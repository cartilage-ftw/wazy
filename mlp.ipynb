{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax_unirep import get_reps\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-model mlp\n",
    "# input\n",
    "# model_batch X sequence_batch X 1900\n",
    "# output\n",
    "# model_batch X sequence_batch X 2 (mean, std)\n",
    "def forward(x):\n",
    "    mlp = hk.nets.MLP([1900, 256, 32, 2])\n",
    "    return mlp(x)\n",
    "\n",
    "forward = hk.transform(forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1900)\n"
     ]
    }
   ],
   "source": [
    "# make data\n",
    "seqs = ['AAAAA', 'AAAAAA', 'AAAAAAA', 'AAAAAAAA']\n",
    "labels = jnp.array([5.4, 10.2, 25.0, 27.3])\n",
    "unirep_seq = get_reps(seqs)[0]\n",
    "print(unirep_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data duplicate model_batch times\n",
    "#deep_ensemble_unirep_seq = unirep_seq[jnp.newaxis, ...]\n",
    "ensemble_unirep_seq = jax.lax.broadcast(unirep_seq, (5,))\n",
    "ensemble_labels = jax.lax.broadcast(labels, (5,))[...,jnp.newaxis]\n",
    "#deep_ensemble_unirep_seq = unirep_seq[jnp.newaxis, ...]\n",
    "#print(deep_ensemble_unirep_seq.shape)\n",
    "#print(ensemble_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(37)\n",
    "model_batch = 5\n",
    "seq_batch = 4 # nothing related to the seqprop sequence batch, it's training batch\n",
    "#x = jnp.ones([model_batch, seq_batch, 1900]) # input\n",
    "#params = forward.init(rng, jnp.ones([1900]))\n",
    "#outs = forward.apply(params, rng, ensemble_unirep_seq)\n",
    "batch_keys = jax.random.split(rng, num=model_batch)\n",
    "batch_keys = jnp.reshape(batch_keys, (model_batch, -1))\n",
    "#batch_keys.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_ensemble_loss(params, ins, labels): # labels are in batches\n",
    "    # in batches\n",
    "    outs = forward.apply(params, rng, ins)\n",
    "    means = outs[0]\n",
    "    #print(means)\n",
    "    stds = outs[1]\n",
    "    n_log_likelihoods = 0.5*jnp.log(jnp.abs(stds)) + 0.5*(labels-means)**2/jnp.abs(stds)\n",
    "    #print(n_log_likelihoods[0])\n",
    "    return n_log_likelihoods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_loss_func(params, seqs, labels, loss_func):\n",
    "    epsilon = 1e-3\n",
    "    grad_inputs = jax.grad(loss_func, 1)(params, seqs, labels)\n",
    "    seqs_ = seqs + epsilon * jnp.sign(grad_inputs)\n",
    "    \n",
    "    return loss_func(params, seqs, labels) + loss_func(params, seqs_, labels)\n",
    "\n",
    "def train_mlp(key, seqs, labels):\n",
    "    learning_rate = 1e-2\n",
    "    n_training_steps = 10\n",
    "    opt_init, opt_update = optax.chain(\n",
    "        optax.scale_by_adam(b1=0.9, b2=0.999, eps=1e-4),\n",
    "        optax.scale(-learning_rate)) # minus sign -- minimizing the loss\n",
    "\n",
    "  # Initialise the model's parameters and the optimiser's state.\n",
    "  # The `state` of an optimiser contains all statistics used by the\n",
    "  # stateful transformations in the `chain` (in this case just `scale_by_adam`).\n",
    "    key, key2 = jax.random.split(key, num=2)\n",
    "    params = forward.init(key, jax.random.normal(key2, shape=(1900,)))\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    loss_trace = []\n",
    "    for step in range(n_training_steps):\n",
    "        # generate adversarial example\n",
    "        #loss_func = adv_loss_func(params, seqs, labels, deep_ensemble_loss)\n",
    "        \n",
    "        loss, grad = jax.value_and_grad(adv_loss_func)(params, seqs, labels, deep_ensemble_loss)\n",
    "        loss_trace.append(loss)\n",
    "        print(f'Loss[{step}] = {loss}')\n",
    "        updates, opt_state = opt_update(grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "    outs = forward.apply(params, key, seqs)\n",
    "    return loss_trace, outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch twice with vmap\n",
    "b_train_mlp = jax.vmap(train_mlp, (None, 0, 0),(0, 0))\n",
    "bb_train_mlp = jax.vmap(b_train_mlp, (0, 0, 0),(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss[0] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[4.0061343e+03, 8.4170898e+03, 3.8728430e+04, 4.0700961e+04],\n",
      "                                       [9.3068975e+03, 3.2290648e+04, 3.4838169e+05, 7.1373380e+06],\n",
      "                                       [2.7320386e+02, 1.0468959e+03, 6.7169062e+03, 8.5491191e+03],\n",
      "                                       [5.5152852e+03, 9.6688809e+03, 4.3855629e+04, 4.9892555e+04],\n",
      "                                       [1.3133713e+03, 6.5267178e+03, 4.8829848e+04, 6.6872117e+04]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[1] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 4.6296444,  3.2547464, 14.665989 , 17.375626 ],\n",
      "                                       [27.640383 , 34.48502  , 66.94472  , 74.64842  ],\n",
      "                                       [ 4.9313097,  8.336259 , 22.357395 , 26.116577 ],\n",
      "                                       [ 3.045517 ,  3.3770561, 24.459278 , 29.93084  ],\n",
      "                                       [ 7.02098  ,  9.454016 , 29.937572 , 34.73481  ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[2] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 8.54232  , 15.822558 , 10.497005 , 11.513052 ],\n",
      "                                       [37.35614  , 40.66091  , 53.91066  , 66.16713  ],\n",
      "                                       [ 9.194082 ,  6.6920986,  7.345638 ,  8.286055 ],\n",
      "                                       [ 6.333131 , 37.016678 , 10.406084 ,  5.2040033],\n",
      "                                       [ 4.934685 ,  4.8270416, 12.318332 ,  8.332472 ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[3] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 9.802218 , 18.349455 , 22.268778 , 28.469067 ],\n",
      "                                       [36.211655 , 37.8914   , 46.191906 , 71.61064  ],\n",
      "                                       [ 6.238469 ,  8.417562 ,  7.63767  ,  6.9225216],\n",
      "                                       [17.367401 , 30.455128 , 29.239948 , 15.43997  ],\n",
      "                                       [ 7.73755  ,  9.861872 ,  5.6084833,  7.95735  ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[4] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 6.5283356, 13.954835 , 23.18866  , 35.92597  ],\n",
      "                                       [22.788036 , 23.952686 , 32.260864 , 73.57631  ],\n",
      "                                       [ 8.291319 ,  6.66945  , 12.406541 , 13.399667 ],\n",
      "                                       [12.090227 , 23.509766 , 33.550385 , 19.328825 ],\n",
      "                                       [ 6.47984  ,  8.618607 , 14.126492 , 16.155737 ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[5] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 8.144034 ,  7.784248 , 17.491882 , 37.360214 ],\n",
      "                                       [ 8.083029 ,  8.669653 , 15.7278805, 69.388016 ],\n",
      "                                       [ 8.432091 ,  7.598548 , 10.088337 , 13.129347 ],\n",
      "                                       [ 5.905076 , 17.129065 , 30.03283  , 15.230709 ],\n",
      "                                       [ 6.796132 ,  6.4353504, 12.31031  , 14.719797 ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[6] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 7.7909756,  7.2003794, 10.633707 , 37.804993 ],\n",
      "                                       [11.732101 , 10.386457 ,  6.426222 , 58.841858 ],\n",
      "                                       [ 7.06838  ,  8.09812  ,  7.1741905,  9.345161 ],\n",
      "                                       [ 9.5754795, 11.229102 , 21.820791 ,  8.530398 ],\n",
      "                                       [ 7.764282 ,  7.1097975,  8.12388  ,  9.071072 ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[7] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 6.662633 , 12.968293 ,  6.4936028, 31.72969  ],\n",
      "                                       [23.378708 , 23.41454  , 14.972067 , 43.30419  ],\n",
      "                                       [ 9.069767 ,  7.392827 ,  8.466417 ,  7.2360983],\n",
      "                                       [ 9.962742 ,  7.140543 , 14.328824 ,  6.772444 ],\n",
      "                                       [ 6.948844 ,  8.474495 ,  6.9407606,  7.031073 ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[8] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 8.770875 ,  9.283936 , 12.588268 , 21.309757 ],\n",
      "                                       [17.682785 , 18.889324 , 25.08626  , 25.87652  ],\n",
      "                                       [ 8.406734 ,  8.042344 , 10.355856 ,  8.65398  ],\n",
      "                                       [ 6.683031 ,  6.8330593,  7.606718 , 10.862508 ],\n",
      "                                       [ 7.5694647,  8.043961 ,  8.40152  , 10.114048 ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n",
      "Loss[9] = Traced<ShapedArray(float32[])>with<BatchTrace(level=2/0)>\n",
      "  with val = Traced<ShapedArray(float32[4])>with<BatchTrace(level=1/0)>\n",
      "               with val = DeviceArray([[ 7.9756436,  6.7117877, 15.235121 , 11.168828 ],\n",
      "                                       [ 9.150366 ,  9.951285 , 21.554764 , 11.6343975],\n",
      "                                       [ 7.7829866,  8.572395 ,  9.047752 , 10.849306 ],\n",
      "                                       [ 8.068563 , 11.101926 ,  7.865016 , 12.362549 ],\n",
      "                                       [ 8.054464 ,  7.144906 ,  9.066984 , 12.515652 ]],            dtype=float32)\n",
      "                    batch_dim = 0\n",
      "       batch_dim = 0\n"
     ]
    }
   ],
   "source": [
    "# call training process\n",
    "#params, loss_trace, outs = b_train_mlp(rng, unirep_seq, labels)\n",
    "loss_trace, outs= bb_train_mlp(batch_keys, ensemble_unirep_seq, ensemble_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([  0.06314812,   9.401098  ,   3.087869  , -11.946028  ], dtype=float32),\n",
       " DeviceArray([1234.2212, 1737.8561, 3502.8018, 1322.3152], dtype=float32))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joint distribution across models\n",
    "#def model_stack(batch_keys, ensemble_unirep_seq, ensemble_labels):\n",
    "def model_stack(outs):\n",
    "    #_, outs = bb_train_mlp(batch_keys, ensemble_unirep_seq, ensemble_labels)\n",
    "    mu = jnp.mean(outs[:,:,0], axis=0)\n",
    "    std = jnp.mean(outs[:,:,1] + outs[:,:,0]**2,axis=0) - mu**2\n",
    "    return mu, std\n",
    "\n",
    "#print(loss_trace)\n",
    "#print(outs)\n",
    "#print(params)\n",
    "model_stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate(params, key, X):\n",
    "    predict, std = forward.apply(params, key, seqs)\n",
    "    return predict, std\n",
    "\n",
    "def acquisition(X, Xsamples, model):\n",
    "    # calculate the best surrogate score found so far\n",
    "    yhat, _ = surrogate(model, X)\n",
    "    best = max(yhat)\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    mu, std = surrogate(model, Xsamples)\n",
    "    mu = mu[:, 0]\n",
    "    # calculate the probability of improvement\n",
    "    probs = norm.cdf((mu - best) / (std+1E-9))\n",
    "    return probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpdesign",
   "language": "python",
   "name": "alpdesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
