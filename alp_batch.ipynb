{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214f9b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import jax\n",
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import optimizers\n",
    "from functools import partial\n",
    "import jax_unirep\n",
    "from jax_unirep.layers import AAEmbedding, mLSTM, mLSTMAvgHidden\n",
    "from jax_unirep.utils import load_params, load_embedding, seq_to_oh\n",
    "from jax_unirep.utils import *\n",
    "from jax_unirep import get_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846e4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET_Unirep = ['-','M','R','H','K','D','E','S','T','N','Q','C','U','G','P','A','V','I','F','Y','W','L','O','X','Z','B','J','start','stop']\n",
    "ALPHABET = ['A','R','N','D','C','Q','E','G','H','I', 'L','K','M','F','P','S','T','W','Y','V']\n",
    "def vectorize(pep):\n",
    "    '''Takes a string of amino acids and encodes it to an L x 20 one-hot vector,\n",
    "    where L is the length of the peptide.'''\n",
    "    vec = jnp.zeros((len(pep), 20))\n",
    "    for i, letter in enumerate(pep):\n",
    "        vec = jax.ops.index_update(vec, jax.ops.index[i, ALPHABET.index(letter)], 1.)\n",
    "    return vec\n",
    "\n",
    "def vec_to_seq(pep_vector):  # From Rainier's code\n",
    "    seq = ''\n",
    "    # expect a 2D numpy array (pep_length x 20), give the string it represents\n",
    "    for letter in pep_vector[:int(jnp.sum(pep_vector))]:\n",
    "        idx = jnp.argmax(letter)\n",
    "        if letter[idx] == 0:\n",
    "            break\n",
    "        seq += ALPHABET[idx]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a22fa",
   "metadata": {},
   "source": [
    "### Unirep related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b277d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_jax_unirep(ohc_seq):\n",
    "    emb_params = load_embedding()\n",
    "    seq_embeddings = []\n",
    "    for oh_vec in ohc_seq:\n",
    "        seq_embedding = jnp.squeeze(jnp.stack([jnp.matmul(oh_vec, emb_params)], axis=0))\n",
    "        seq_embeddings.append(seq_embedding)\n",
    "    seq_embeddings = jnp.stack(seq_embeddings, axis=0)\n",
    "    _, mLSTM_apply_fun = mLSTM(1900)\n",
    "    weight_params = load_params()[1]\n",
    "    def apply_fun_vmapped(x):\n",
    "        return mLSTM_apply_fun(params=weight_params, inputs=x)\n",
    "    h_final, c_final, outputs = vmap(apply_fun_vmapped)(seq_embeddings)\n",
    "    #h_final, _, outputs = jax.vmap(partial(mLSTM_apply_fun, weight_params))(seq_embedding)\n",
    "    h_avg = jnp.mean(outputs, axis=1)\n",
    "    return h_avg\n",
    "\n",
    "def index_trans(oh, alphabet, alphabet_unirep):\n",
    "    matrix = jnp.zeros((len(alphabet), 26))\n",
    "    for idx, aa in enumerate(alphabet):\n",
    "        matrix = jax.ops.index_update(matrix, tuple([idx, alphabet_unirep.index(aa)]), 1.)\n",
    "    start_char = jnp.zeros((1, 26))\n",
    "    start_char = jax.ops.index_update(start_char, (0, 24), 1.)\n",
    "    oh_unirep = jnp.einsum('ij,jk->ik', oh, matrix)\n",
    "    oh_unirep = jnp.vstack((start_char, oh_unirep))\n",
    "    return oh_unirep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c300b2",
   "metadata": {},
   "source": [
    "### Sampling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae1b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.partial(jax.custom_jvp, nondiff_argnums=(0,))\n",
    "def disc_ss(key, logits):\n",
    "    key, sub_key = jax.random.split(key, num=2)\n",
    "    sampled_onehot = jax.nn.one_hot(jax.random.categorical(key, logits), logits.shape[-1])\n",
    "    return sampled_onehot\n",
    "\n",
    "# customized gradient for back propagation\n",
    "@disc_ss.defjvp\n",
    "def disc_ss_jvp(key, primals, tangents):\n",
    "    key, subkey = jax.random.split(key, num=2)\n",
    "    logits = primals[0]\n",
    "    logits_dot = tangents[0]\n",
    "    primal_out = disc_ss(key, logits)\n",
    "    _, tangent_out = jax.jvp(jax.nn.softmax, primals, tangents)\n",
    "    return primal_out, tangent_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f53085",
   "metadata": {},
   "source": [
    "### Norm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bedddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_layer(logits, r, b):\n",
    "    epsilon = 1e-5\n",
    "    M, N = jnp.shape(logits)\n",
    "    miu = jnp.sum(logits) / (M*N)\n",
    "    std = jnp.sqrt(jnp.sum((logits - miu)**2) / (M*N))\n",
    "    norm_logits = (logits - miu) / (std**2 + epsilon)\n",
    "    scaled_logits = norm_logits * r + b\n",
    "    return scaled_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bb925",
   "metadata": {},
   "source": [
    "### Batch Version Forward Seqprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52354b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_seqprop_batch(key, logits_batch, r_batch, b_batch):\n",
    "    batch_size = len(logits_batch)\n",
    "    sampled_vec_batch = []\n",
    "    norm_logits_batch = []\n",
    "    for i in range(batch_size):\n",
    "        logits = logits_batch[i]\n",
    "        r = r_batch[i]\n",
    "        b = b_batch[i]\n",
    "        norm_logits = norm_layer(logits, r, b) # same dimension as logits\n",
    "        sampled_vec = disc_ss(key, norm_logits)\n",
    "        sampled_vec_batch.append(sampled_vec)\n",
    "        norm_logits_batch.append(norm_logits)\n",
    "        \n",
    "    return sampled_vec_batch, norm_logits_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e4b5a",
   "metadata": {},
   "source": [
    "### Batch Version Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd7b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_batch(target_rep, sampled_vec_batch):\n",
    "    batch_size = len(sampled_vec_batch)\n",
    "    sampled_vec_unirep_batch = []\n",
    "    losses = []\n",
    "    for i in range(batch_size):\n",
    "        sampled_vec_unirep = index_trans(sampled_vec_batch[i], ALPHABET, ALPHABET_Unirep)\n",
    "        sampled_vec_unirep_batch.append(sampled_vec_unirep)\n",
    "    h_avg= differentiable_jax_unirep(sampled_vec_unirep_batch)\n",
    "    for i in range(h_avg.shape[0]):\n",
    "        losses.append(1-jnp.sum(jnp.vdot(h_avg[i], target_rep))/jnp.sqrt(jnp.sum(h_avg[i]**2)*jnp.sum(target_rep**2)))\n",
    "    # loss = jnp.mean(((target_rep - h_avg)/target_rep)**2)   # mean square error\n",
    "    #loss = 1-jnp.sum(jnp.vdot(h_avg, target_rep))/jnp.sqrt(jnp.sum(h_avg**2)*jnp.sum(target_rep**2))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff26ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_loss_func(key, logits, r, b, target_rep):\n",
    "    sampled_vec, _ = forward_seqprop_batch(key, logits, r, b)\n",
    "    return loss_func_batch(target_rep, sampled_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4649a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_char = ['G','I','G','A','V','L','K','V','L','T','T','G','L','P','A','L','I','S','W','I','K','R','K','R','Q','Q']\n",
    "oh_vec = vectorize(target_char)\n",
    "target_seq = ['GIGAVLKVLTTGLPALISWIKRKRQQ']\n",
    "target_rep = get_reps(target_seq)[0]\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# batch the logits 32\n",
    "batch_num = 32\n",
    "logits_batch = []\n",
    "r_batch = []\n",
    "b_batch = []\n",
    "# make batched logits\n",
    "for _ in range(batch_num):\n",
    "    key, logits_key, r_key, b_key = jax.random.split(key, num=4)\n",
    "    logits_batch.append(jax.random.normal(logits_key, shape=jnp.shape(oh_vec)))\n",
    "    r_batch.append(jax.random.normal(r_key))\n",
    "    b_batch.append(jax.random.normal(b_key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1509fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_vec_batch, norm_logits_batch = forward_seqprop_batch(key, logits_batch, r_batch, b_batch)\n",
    "loss = loss_func_batch(target_rep, sampled_vec_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6ba4e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeviceArray(0.19891185, dtype=float32), DeviceArray(0.24761719, dtype=float32), DeviceArray(0.247576, dtype=float32), DeviceArray(0.3009013, dtype=float32), DeviceArray(0.2752024, dtype=float32), DeviceArray(0.30030912, dtype=float32), DeviceArray(0.28053582, dtype=float32), DeviceArray(0.24470818, dtype=float32), DeviceArray(0.13517243, dtype=float32), DeviceArray(0.2691852, dtype=float32), DeviceArray(0.28933328, dtype=float32), DeviceArray(0.23477793, dtype=float32), DeviceArray(0.27657723, dtype=float32), DeviceArray(0.2933095, dtype=float32), DeviceArray(0.2606218, dtype=float32), DeviceArray(0.28960413, dtype=float32), DeviceArray(0.3630033, dtype=float32), DeviceArray(0.29853022, dtype=float32), DeviceArray(0.22240508, dtype=float32), DeviceArray(0.38732547, dtype=float32), DeviceArray(0.20633137, dtype=float32), DeviceArray(0.31671095, dtype=float32), DeviceArray(0.23193097, dtype=float32), DeviceArray(0.2935465, dtype=float32), DeviceArray(0.27611536, dtype=float32), DeviceArray(0.3067733, dtype=float32), DeviceArray(0.24538743, dtype=float32), DeviceArray(0.3091526, dtype=float32), DeviceArray(0.29629308, dtype=float32), DeviceArray(0.2857483, dtype=float32), DeviceArray(0.26021296, dtype=float32), DeviceArray(0.21411681, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb7549e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seqprop_batch(key, target_rep, init_logits_batch, init_r_batch, init_b_batch, iter_num=600):\n",
    "    opt_init, opt_update, get_params = optimizers.adam(step_size=1e-2, b1=0.9, b2=0.8)\n",
    "    #opt_init, opt_update, get_params = optimizers.adagrad(step_size=1e-2)\n",
    "    opt_state = opt_init((init_logits_batch, init_r_batch, init_b_batch)) # initial state\n",
    "    logits_trace = []\n",
    "    loss_trace = []\n",
    "\n",
    "    @jax.jit\n",
    "    def step(key, i, opt_state):\n",
    "        key, subkey = jax.random.split(key, num=2)\n",
    "        p = get_params(opt_state)\n",
    "        logits, r, b = p\n",
    "        sampled_vec, norm_logits = forward_seqprop_batch(key, logits, r, b)\n",
    "        loss = loss_func_batch(target_rep, sampled_vec)\n",
    "        #g = jax.grad(g_loss_func, (1,2,3))(key, logits, r, b, target_rep)\n",
    "        g = jax.jacfwd(g_loss_func, (1,2,3))(key, logits, r, b, target_rep)\n",
    "        print(g)\n",
    "        return opt_update(i, g, opt_state), loss\n",
    "\n",
    "    for step_idx in range(iter_num):\n",
    "        print(step_idx)\n",
    "        opt_state, loss = step(key, step_idx, opt_state)\n",
    "        print(loss)\n",
    "        loss_trace.append(loss)\n",
    "        mid_logits, mid_r, mid_b = get_params(opt_state)\n",
    "        logits_trace.append(mid_logits)\n",
    "    final_logits, final_r, final_b = get_params(opt_state)\n",
    "    sampled_vec, _ = forward_seqprop(key, final_logits, final_r, final_b)\n",
    "    return sampled_vec, final_logits, logits_trace, loss_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "sampled_vec, final_logits, logits_trace, loss_trace = train_seqprop_batch(key, target_rep, logits_batch, r_batch, b_batch, iter_num = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915db73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a225a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b92ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpdesign",
   "language": "python",
   "name": "alpdesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
