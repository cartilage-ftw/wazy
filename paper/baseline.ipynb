{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from alpdesign.utils import *\n",
    "from alpdesign.mlp import *\n",
    "from jax_unirep import get_reps\n",
    "import alpdesign\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax_unirep\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import functools\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_list = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V','B','Z','X','*']\n",
    "blosum92 = np.loadtxt(\"./blosum62.txt\", dtype='i', delimiter=' ')\n",
    "def blosum(seq1, seq2):\n",
    "    seqlist1 = list(seq1)\n",
    "    seqlist2 = list(seq2)\n",
    "    score = 0.\n",
    "    for i in range(len(seqlist1)):\n",
    "        idx1 = AA_list.index(seqlist1[i])\n",
    "        idx2 = AA_list.index(seqlist2[i])\n",
    "        score += blosum92[idx1][idx2]\n",
    "    return jax.nn.sigmoid(score/len(seqlist1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33181223 0.8021839  0.9241418  0.880797   0.21416503 0.2890505\n",
      " 0.26894143 0.3543437  0.21416503 0.33181223 0.66818774 0.21416503]\n"
     ]
    }
   ],
   "source": [
    "target_seq = 'GIGAVLKVLT'\n",
    "oh_vec = encode_seq(list(target_seq))\n",
    "oh_unirep = seq2useq(oh_vec)\n",
    "target_rep = differentiable_jax_unirep(oh_unirep)\n",
    "'''\n",
    "seqs = ['GIGAVLKVLKAGLPALIVTLKRKIVQ',\n",
    "       'PPGATLKKHTTGSVALISWIWARIQQ',\n",
    "       'GIGAVLKVLTTGLKTLISAAKRKRAA',\n",
    "       'HAPPVLKVLTTGLAPPLVWIKRKRTH',\n",
    "       'GIGAVLUIHKLSSVAAWRPPKRKRQQ',\n",
    "       'PTWIIFLKAQWEQHSNLTNMRTFPEV',\n",
    "        'TISHFVCNHDICAWIKDMQAMQIKMC',\n",
    "        'CESWLWKRLFDGHADRWRSMPDYPIW',\n",
    "        'YLVENPLMFPLVAAFIHQWTRQISWH',\n",
    "        'QTEERLEAQISIYYIGAWSHYKVTDE']\n",
    "'''\n",
    "\n",
    "seqs = ['SSVAAWRPPK',\n",
    "       'PPGATLKKHT',\n",
    "       'LTGAVLKVLK',\n",
    "       'HAPPVLKVLT',\n",
    "       'AWRPPKRKRQ',\n",
    "       'PTWIIFLKAQ',\n",
    "       'TISHFVCNHD',\n",
    "       'CESWLWKRLF',\n",
    "       'YLVENPLMFP',\n",
    "       'QTEERLEAQI',\n",
    "       'GAWSHYKVTD',\n",
    "       'HADRWRSMPD']\n",
    "reps = get_reps(seqs)[0]\n",
    "labels = []\n",
    "for seq in seqs:\n",
    "    labels.append(blosum(target_seq, seq))\n",
    "labels = np.array(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "class NaiveBlock(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = hk.nets.MLP((256, 128, 64, 1,))(x)\n",
    "        return out\n",
    "    \n",
    "def naive_forward(x):\n",
    "    f = NaiveBlock()\n",
    "    return f(x)\n",
    "naive_forward_t = hk.without_apply_rng(hk.transform(naive_forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _naive_loss(forward, params, seq, label):\n",
    "    yhat = forward(params, seq) #scalar\n",
    "    return (label-yhat)**2\n",
    "\n",
    "def _shuffle_in_unison(key, a, b):\n",
    "    # NOTE to future self: do not try to rely on keys being same\n",
    "    # something about shape of arrays makes shuffle not the same\n",
    "    assert len(a) == len(b)\n",
    "    p = jax.random.permutation(key, len(a))\n",
    "    return jnp.array([a[i] for i in p]), jnp.array([b[i] for i in p])\n",
    "\n",
    "\n",
    "def _fill_to_batch(x, y, key, batch_size):\n",
    "    if len(y) >= batch_size:\n",
    "        return x, y\n",
    "    i = jax.random.choice(key, jnp.arange(len(y)),\n",
    "                          shape=(batch_size,), replace=True)\n",
    "    x = x[i, ...]\n",
    "    y = y[i, ...]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def naive_train(key, forward_t, seqs, labels, val_seqs=None, val_labels=None, params=None, epochs=3, batch_size=8, learning_rate=1e-2):\n",
    "    opt_init, opt_update = optax.chain(\n",
    "        optax.scale_by_adam(b1=0.8, b2=0.9, eps=1e-4),\n",
    "        optax.scale(-learning_rate)  # minus sign -- minimizing the loss\n",
    "    )\n",
    "\n",
    "    key, bkey = jax.random.split(key)\n",
    "\n",
    "    # fill in seqs/labels if too small\n",
    "    seqs, labels = _fill_to_batch(seqs, labels, bkey, batch_size)\n",
    "   \n",
    "        \n",
    "    if params == None:\n",
    "        params = forward_t.init(key, seqs[0])\n",
    "  \n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    # wrap loss in batch/sum\n",
    "    loss_ = partial(_naive_loss, forward_t.apply)\n",
    "    loss_fxn = lambda *args: jnp.mean(jax.vmap(loss_, in_axes=(None, 0, 0))(*args))\n",
    "    #loss_fxn = jnp.mean(jax.vmap(loss_, in_axes=(None, 0, 0)))\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(opt_state, params, seqs, labels):\n",
    "        loss, grad = jax.value_and_grad(loss_fxn, 0)(\n",
    "            params, seqs, labels)\n",
    "        updates, opt_state = opt_update(grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return opt_state, params, loss\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle seqs and labels\n",
    "        key, key_ = jax.random.split(key, num=2)\n",
    "        shuffle_seqs, shuffle_labels = _shuffle_in_unison(key, seqs, labels)\n",
    "        for i in range(0, len(shuffle_labels) // batch_size):\n",
    "            seq = shuffle_seqs[i:(i+1) * batch_size]\n",
    "            label = shuffle_labels[i:(i+1) * batch_size]\n",
    "            opt_state, params, loss = train_step(opt_state, params, seq, label)\n",
    "            losses.append(loss)\n",
    "        # compute validation loss\n",
    "        if val_seqs is not None:\n",
    "            val_loss = 0.\n",
    "            for i in range(0, len(val_labels) // batch_size):\n",
    "                seq = shuffle_seqs[i:(i+1) * batch_size]\n",
    "                label = shuffle_seqs[i:(i+1) * batch_size]\n",
    "                val_loss += loss_fxn(\n",
    "                    params,\n",
    "                    val_seqs[i:(i+1) * batch_size],\n",
    "                    val_labels[i:(i+1) * batch_size])\n",
    "            val_loss = val_loss/len(val_labels) * batch_size\n",
    "            #batch_loss += loss\n",
    "            val_losses.append(val_loss)\n",
    "    return (params, losses) if val_seqs is None else (params, losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f63c4088d10>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcOUlEQVR4nO3de5CcV53e8e/z9mVmNLrZlnxBslcClAUBNjgTA+HqhIvNJityN0WWLQKrOMHZUAm1MbUptgiVP3Yru0lImVU5rLeWYr3ObsC1ChHYXsIuCV7WGoHvWLZsbCTLtkbWXXPryy9/vO/0tLpbnpY045HOPJ+qqek+7/t2n9Pqfubo9Pueo4jAzMzSlS12BczMbGE56M3MEuegNzNLnIPezCxxDnozs8SVF7sCvaxZsyY2bNiw2NUwM7tg7Nq162BErO217bwM+g0bNjA6OrrY1TAzu2BIeu502zx0Y2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolLKuj/23ef4i+eHFvsapiZnVeSCvqv/PnT/GDPwcWuhpnZeSWpoM8EzaYXUjEza5dY0AvnvJnZqZIKegmaXhrRzOwUfQW9pBsk7Za0R9KtPbZvkfSwpAcljUp6d7/HzqcsE14D18zsVHMGvaQScBtwI7AZ+JikzR27fRe4JiLeCvwz4KtncOy8EXjoxsysQz89+uuAPRHxTERMA3cBW9p3iIgTMduVHgai32PnUyYROOnNzNr1E/TrgL1t9/cVZaeQ9PckPQH8b/Jefd/HFsdvLYZ9RsfGzu5cePnLWDOzLv0EvXqUdcVpRNwdEW8APgp86UyOLY6/PSJGImJk7dqei6TMKRMeozcz69BP0O8Drmy7vx7Yf7qdI+L7wOskrTnTY89VJtFsLtSjm5ldmPoJ+p3AJkkbJVWBm4Dt7TtIer0kFbevBarAy/0cO58yn15pZtZlzjVjI6Iu6RbgHqAE3BERj0m6udi+DfgHwCck1YAJ4J8UX872PHaB2uIxejOzHvpaHDwidgA7Osq2td3+TeA3+z12oUj4rBszsw5JXRmbSXjkxszsVIkFvcfozcw6JRb0HqM3M+uUVNB7UjMzs26JBb0nNTMz65RU0OdXxi52LczMzi+JBb08dGNm1iGpoPcFU2Zm3ZIKek9qZmbWLamgz8+6WexamJmdX5IKeo/Rm5l1Syro5SkQzMy6JBX0ngLBzKxbYkHvHr2ZWafEgt49ejOzTkkFvfCXsWZmndIKek+BYGbWJamg9xi9mVm3tII+8xi9mVmntILeF0yZmXVJKug9qZmZWbe0gh5PamZm1impoM8Ejnkzs1P1FfSSbpC0W9IeSbf22P5xSQ8XP/dLuqZt27OSHpH0oKTR+ax8J4/Rm5l1K8+1g6QScBvwQWAfsFPS9oh4vG23nwLvi4jDkm4Ebgfe3rb9+og4OI/1Pl1daTYX+lnMzC4s/fTorwP2RMQzETEN3AVsad8hIu6PiMPF3R8C6+e3mv3xFAhmZt36Cfp1wN62+/uKstP5FPDttvsB3Ctpl6StpztI0lZJo5JGx8bG+qhWN18wZWbWbc6hG/KTWTr1jFNJ15MH/bvbit8VEfslXQrcJ+mJiPh+1wNG3E4+5MPIyMhZxbXcozcz69JPj34fcGXb/fXA/s6dJF0NfBXYEhEvz5RHxP7i9wHgbvKhoAWRST7rxsysQz9BvxPYJGmjpCpwE7C9fQdJVwHfBH4pIp5sKx+WtGLmNvAh4NH5qnwn9+jNzLrNOXQTEXVJtwD3ACXgjoh4TNLNxfZtwBeAS4CvSAKoR8QIcBlwd1FWBu6MiO8sSEvwGL2ZWS/9jNETETuAHR1l29pufxr4dI/jngGu6SxfKD7rxsysW2JXxvqCKTOzTkkFPcIXTJmZdUgq6DP1OhPUzGxpSyzoPUZvZtYpsaD3GL2ZWaekgt4Lj5iZdUsq6DN54REzs05JBX1+Zexi18LM7PySVNDnV8Y66c3M2iUX9O7Rm5mdKqmg96RmZmbdkgp6T2pmZtYtsaB3j97MrFNSQS9fMGVm1iWxoMdDN2ZmHZIKeo/Rm5l1SyzoPUZvZtYpsaD3GL2ZWaekgl54CgQzs05pBX2x8IinQTAzm5VU0GetoF/kipiZnUcSC/r8t8fpzcxmpRX0RdJ7nN7MbFZfQS/pBkm7Je2RdGuP7R+X9HDxc7+ka/o9dj7JPXozsy5zBr2kEnAbcCOwGfiYpM0du/0UeF9EXA18Cbj9DI6dN8Jj9GZmnfrp0V8H7ImIZyJiGrgL2NK+Q0TcHxGHi7s/BNb3e+x8mhmjD5z0ZmYz+gn6dcDetvv7irLT+RTw7TM9VtJWSaOSRsfGxvqoVreZs248Rm9mNqufoFePsp5RKul68qD/d2d6bETcHhEjETGydu3aPqrV6/nz3x6jNzObVe5jn33AlW331wP7O3eSdDXwVeDGiHj5TI6dL63z6JsL9QxmZheefnr0O4FNkjZKqgI3Advbd5B0FfBN4Jci4skzOXY++Tx6M7Nuc/boI6Iu6RbgHqAE3BERj0m6udi+DfgCcAnwlWIagnoxDNPz2AVqS2sKBAe9mdmsfoZuiIgdwI6Osm1ttz8NfLrfYxfK7Fk3ZmY2I6krY92jNzPrllTQe1IzM7NuiQV9/ts9ejOzWYkFvS+YMjPrlFTQz1ye1XTSm5m1JBX0Mz16MzOblVjQ5789Rm9mNiuxoPcYvZlZp6SCfmbkpuGkNzNrSSroZ8+jd9Cbmc1IKuhLXjPWzKxLUkGfeejGzKxLYkHvuW7MzDolFfSzQzcOejOzGUkF/UyP3kM3Zmaz0gp6fxlrZtYlraD3lbFmZl2SCvqSh27MzLokFfSZv4w1M+uSVtDPnF7ZXOSKmJmdR5IK+lLRGvfozcxmJRX0M4uDNxz0ZmYtfQW9pBsk7Za0R9KtPba/QdJfSpqS9LmObc9KekTSg5JG56vivZRaQzcOejOzGeW5dpBUAm4DPgjsA3ZK2h4Rj7ftdgj4VeCjp3mY6yPi4DnWdU6ej97MrFs/PfrrgD0R8UxETAN3AVvad4iIAxGxE6gtQB37lhWt8emVZmaz+gn6dcDetvv7irJ+BXCvpF2Stp5J5c7UzFw3no/ezGzWnEM3QK8Vt88kSd8VEfslXQrcJ+mJiPh+15PkfwS2Alx11VVn8PCzMn8Za2bWpZ8e/T7gyrb764H9/T5BROwvfh8A7iYfCuq13+0RMRIRI2vXru334U/hMXozs279BP1OYJOkjZKqwE3A9n4eXNKwpBUzt4EPAY+ebWXn0prrxklvZtYy59BNRNQl3QLcA5SAOyLiMUk3F9u3SbocGAVWAk1JnwU2A2uAu4vz28vAnRHxnQVpCbNj9P4y1sxsVj9j9ETEDmBHR9m2ttsvkg/pdDoGXHMuFTwTXmHKzKxbUlfGelIzM7NuSQV9yV/Gmpl1SSroZ76M9Ri9mdmstILeQzdmZl3SCnpPamZm1iWpoG8tJeicNzNrSSroZyY181w3Zmaz0gp6Lw5uZtYlqaAvZT690sysU1JBr5m5bjx0Y2bWklTQlzx0Y2bWJa2g93n0ZmZdkgp6+Tx6M7MuSQU95L1657yZ2azkgj6TlxI0M2uXYNDLQzdmZm3SDHr36M3MWpIL+lImGs3FroWZ2fkjuaDP5NMrzczapRf0mYduzMzaJRf0JclXxpqZtUku6CWfR29m1i65oC9lvjLWzKxdX0Ev6QZJuyXtkXRrj+1vkPSXkqYkfe5Mjp1vJZ9eaWZ2ijmDXlIJuA24EdgMfEzS5o7dDgG/Cvynszh2XknylbFmZm366dFfB+yJiGciYhq4C9jSvkNEHIiInUDtTI+db6VMOOfNzGb1E/TrgL1t9/cVZf3o+1hJWyWNShodGxvr8+G7ZYKTU3X2Hho/68cwM0tJP0GvHmX99pn7PjYibo+IkYgYWbt2bZ8P3y3LxL2Pv8R7fut7PPr80bN+HDOzVPQT9PuAK9vurwf29/n453LsWZlZIBzgrp0/W8inMjO7IPQT9DuBTZI2SqoCNwHb+3z8czn2rJTagn5i2pPemJmV59ohIuqSbgHuAUrAHRHxmKSbi+3bJF0OjAIrgaakzwKbI+JYr2MXqC1APnQzY9qzm5mZzR30ABGxA9jRUbat7faL5MMyfR27kNpynul649V6WjOz81aCV8a29ejr7tGbmSUX9JKHbszM2iUX9KW2oZsf7HmZX/na6OJVxszsPJBe0Gennrp/3+MvLVJNzMzOD8kFffvQjZmZJRj05aw76D/5+w946mIzW7KSC/p1q4e6yr63e4wjE53zrZmZLQ3JBf3rLl3es3yy5nPqzWxpSi7or1m/umf5+HT91a2Imdl5Irmgf+frLuG/f2KEj7/9qlPKT0y5R29mS1NyQQ/wwc2XsWqockrZi0cnF6k2ZmaLK8mgB+g8y/Lmr+/i/qcPLk5lzMwWUbJB32v2Ay9EYmZLUbJB3+yxcOzygUqPPc3M0pZs0Dd6XCA1VE22uWZmp5Vs8vXq0dfqvjrWzJaedIO+R49+0guRmNkSlGzQN3r06Kdqnp/ezJaeZIN+3eplXWWeBsHMlqJkg/5X3rORG950OZBPdJYJpry0oJktQckGfbmUceNb8qCXYLBSco/ezJakZIMeoJzNNm+gnLlHb2ZLUtpBXywg6x69mS1lfQW9pBsk7Za0R9KtPbZL0peL7Q9LurZt27OSHpH0oKRXdaXu9tWm3KM3s6VqzqCXVAJuA24ENgMfk7S5Y7cbgU3Fz1bgdzu2Xx8Rb42IkXOvcv/Kpbx5QgxWSrx4dJJv7Nr3albBzGzR9dOjvw7YExHPRMQ0cBewpWOfLcDXIvdDYLWkK+a5rmesks0O3QxUSjzw7CH+7Z88xIFjnrLYzJaOfoJ+HbC37f6+oqzffQK4V9IuSVtP9ySStkoalTQ6NjbWR7XmNtujh4HSbFOPTXr9WDNbOvoJevUo67zs9JX2eVdEXEs+vPMZSe/t9SQRcXtEjETEyNq1a/uo1txmvowFeLGtF39s0ssKmtnS0U/Q7wOubLu/Htjf7z4RMfP7AHA3+VDQq6JSnF4piZ8dGm+VH5twj97Mlo5+gn4nsEnSRklV4CZge8c+24FPFGffvAM4GhEvSBqWtAJA0jDwIeDReaz/K2rv0bc77h69mS0h5bl2iIi6pFuAe4AScEdEPCbp5mL7NmAH8BFgDzAOfLI4/DLgbuXr+pWBOyPiO/PeitOotAX9/9j6Dh746SF++74nHfRmtqTMGfQAEbGDPMzby7a13Q7gMz2Oewa45hzreNaKPzAIePtrL+Et61fx2/c96S9jzWxJSfrK2M6ZiocqJUqZOD5Z48CxSX7tfz7E3T/2efVmlrakg7514k8xgiOJlYNljk3Uuefxl/jj0X381z97avGqZ2b2Kkg66FcM5ouBv+3Ki04pOz5Z48WjEwBMeP4bM0tcX2P0F6rLVg7yrX/1bl5/6fJW2aUrBtgzdoKsuGr20MlpIqI1nm9mlpqke/QAb163isFKqXX/I2+5gkefP8b/feogALVGsOORF4keSw+amaUg+aDv9L6fz6+6HTs+1Sr7zJ0/4vf+308Xq0pmZgtqyQX9ZSsHW7c3X7GydfsnLxxfjOqYmS24JRf0ywdmv5Z4z6Y1rds+t97MUrXkgr7d33z9Gh794oe5buPFnv/GzJK1pIP+tWuGWT5QZtVQhaMOejNL1JIO+tesHgJg5WCF45N1Dp+cptn02TdmlpYlGfR/cvM7+Y2/u5lScS79yqEyzx+Z4G1fuo+7du6d42gzswvLkgz6v7HhYj75ro2t+6uGKq3bo88dWowqmZktmCUZ9J1WDs4Gfa3hoRszS4uDHjg8Pt26vbdtJSozsxQ46IEPv+lyqqWMd772EvYddtCbWVoc9OTz4Tz5H2/kA5sv4+CJaX70s8Nc88V7+dMHn1/sqpmZnTMHfZt/+NfXs3KwzOe/8QhHJ2p86VuPL3aVzMzOmYO+zaqhCn//2vXsfimf98ZTF5tZChz0Hf7RyPpT7k9Me2ESM7uwOeg7vOk1q3jTa/JZLceOT/HGL3yHz3/zYX72sr+kNbMLk4O+h9/5x2/lP2x5U+vK2T96YC///Ou7PB+OmV2QHPQ9/PzlK/jEOzew699/oFX2kxeOcc0X7+Xbj7ywiDUzMztzfQW9pBsk7Za0R9KtPbZL0peL7Q9LurbfY89nq5dV+fLH3nZK2b/4wx/x+z/walRmduGYc3FwSSXgNuCDwD5gp6TtEdF+7uGNwKbi5+3A7wJv7/PY89ovXvMa3rdpLV//q+e4/+mD/GDPy3zxfz1OJvHev7aWK1YNMlVvAjBcLVEund1/kprNIMtERDBZazJYyYigtYj5iak6B49PceXFywBaw0qnM11vUinptGcO1RtNphtNllXLRAQRMFlvMFVrcnSixvh0g+UDZQ4cn2TTZSuoljKGqvnau2ezmPrTYyd44oXj/O03XspAOWsdP1VvMFAunbLv4ZPTPHdonL/YPcYHNl/KGy9f2XodzlREsO/wBMsHyiwbKHF8sk5E/votHyhTyjTna9mPiekGD+07ws9dsozlA2Wq5YxqKXvF16nWaPLcyyfZuGY50/UmWUbXa3EmIoJ6M6i0vQdPTtWplrNTyprN4OhEjYuGq2f0+DPvwQ1rhrvaUS5ew8U4U63RDMan6wxWSlRKGXsPjbNysMJ0o8klw1Wks6/XdD3/PKxdMdD1nNk5PO6rbc6gB64D9kTEMwCS7gK2AO1hvQX4WuQrbP9Q0mpJVwAb+jj2vLdqWYXPXP96/uX7X8dj+49xy50/4je2P9Zz32o5awV+vt54MLPueJB/GJsBKwbLnJyqM11vsmqowoHjUwwPlGk0gxNTdS4ZrnJ0okallLFisMzYiSkiQIKh4g3djGDVUIVMotEMao0mzeI5D56Y5uLhKpng2GSdVUMVhiolJmsNJqYbHJ+qI8Elw1UOj9dYVilxfKp+2tcgEwxWSgxWShwr3vgzb/HON3tE0IignGVkGZycanDo5Ow0EwPljIuWVZmoNVofomrRnpdPTjNd/OEE+M9/9iSlTFRLGSuHyggx83SlTETAyek8vFcNVfIPYAZC1BpNjozXmKid/sypSkkMVUpI+eOqaE/+G6C9nNbzz+xHUX5kvMaJ4vWTIALWLK+2wn58us5ErcFwtcxQtcShk9PUGk1qjaCciXoxPfaKwTL1RnBxWwiXMrWCtFm8fxrNaL2X8rLg2ESd6UaTy1cOUi/Cb6LWoCSxrFpioFKikoljk3VOTNUZrpZoBlw8XGWq3qBayiiXsu72AY0Inj88Qb0ZrBzM36eN4v03dnyKcpbRiODKi4bIJNpnjIo4df6oztmk2jdH8XmJ00w5NfNvP/P7yHiN45P5614tZ1QycbLjTLmZSQtXDJaLx847VaVMZMUDRQRRvL7tz3/wxBRT9WbrPTpVbyLBsYka1XLGQDljqtZk1bLK7Oe847Gakb/nq+X8PQ6Qqft9BHDxsip/fPM7ezf+HPQT9OuA9rl795H32ufaZ12fxwIgaSuwFeCqq67qo1qvPkm8ed0qvve59/PI80d5aO8RjozXKJcyKiUxPt1gfLrByal664PbHhIwGxTHJ+sMD+SBfWS8xmUrBzk+WaOUictWDua9kqEKzWZwbLLG5auGKElMN/JwFPkb9dhEjWA2DKT8fwWXrxrkxaOTAK2FVSZrDYaqJQbKJVYNVag3m4wdn+Ki4SrHJuqsWz3IiakGa5ZXOXhimjXLqzx/ZIJlRU/+yHiNRjNYPlBufTF96gd6tp0liVqzSbMZDA+UWX/RMtauGOCpl44zVW+2epoXD1d56dgkjWZ+7EXLKlRKGVesHmKq1qAZweHxGuNTdU5MNcg0+5wzH5rhav42Pj5ZK/5XlH/Y8j+SFV6zOn8tVhST19WbTYaqJV48OslgpcR0vdn6cEa0hQ20/lg3m93lQX4ngGXVEpeuGOTYZI3hgTKTtQZHx2utYK6W8z/Y49N1xqcarByqUM7E8sEyJybz98tQtcTRiRrlLH8vzbymzcj/iEsi02xIlJQHVZbl780Vg2UyiZeOTVIt5b34wUpGKcuYrDWYrDWoNYLhgRKXrcxfk/z9N81A8To0ms22ds++1gJ+4S1XMDyQT+kdEQyUS5ycqnPJ8gGOTtRYOVRm3+GJU/5Yzn52Oj5LPT5b7fu2/0Gfrcvsu23m33j1smrrf5qNZjBdb7J8oEyWiclag/HpOo0mlDNxYqreeu2azfyP48wchjOf0Wym3kUdLlpW4fJVg601pavl/H9GA+WMerNJrR5Uyxknp+qtY/LHaesUSEzVGtSbcUqbWn9UCGZerRWD/UTymevnUXv936Tz7+3p9unn2Lww4nbgdoCRkZHzegpJSVy9fjVXr1+92FUxM5tTP0G/D7iy7f56YH+f+1T7ONbMzBZQP98c7gQ2SdooqQrcBGzv2Gc78Ini7Jt3AEcj4oU+jzUzswU0Z48+IuqSbgHuAUrAHRHxmKSbi+3bgB3AR4A9wDjwyVc6dkFaYmZmPanzG/HzwcjISIyOji52NczMLhiSdkXESK9tvjLWzCxxDnozs8Q56M3MEuegNzNL3Hn5ZaykMeC5szx8DXBwHqtzIXCblwa3eWk42zb/XESs7bXhvAz6cyFp9HTfPKfKbV4a3OalYSHa7KEbM7PEOejNzBKXYtDfvtgVWARu89LgNi8N897m5MbozczsVCn26M3MrI2D3swscckE/YW8CPkrkXSHpAOSHm0ru1jSfZKeKn5f1Lbt88VrsFvShxen1udG0pWSvifpJ5Iek/Svi/Jk2y1pUNIDkh4q2vzFojzZNs+QVJL0Y0nfKu4n3WZJz0p6RNKDkkaLsoVtc74w9IX9Qz4F8tPAa8kXO3kI2LzY9Zqntr0XuBZ4tK3st4Bbi9u3Ar9Z3N5ctH0A2Fi8JqXFbsNZtPkK4Nri9grgyaJtybabfDW25cXtCvBXwDtSbnNb2/8NcCfwreJ+0m0GngXWdJQtaJtT6dG3FjCPiGlgZhHyC15EfB841FG8BfiD4vYfAB9tK78rIqYi4qfk6wNc92rUcz5FxAsR8aPi9nHgJ+TrDyfb7sidKO5Wip8g4TYDSFoP/ALw1bbipNt8Ggva5lSC/nSLk6fqsshX8KL4fWlRntzrIGkD8DbyHm7S7S6GMB4EDgD3RUTybQb+C/BrQLOtLPU2B3CvpF2SthZlC9rmhVly/NXX9yLkiUvqdZC0HPgG8NmIOCb1al6+a4+yC67dEdEA3ippNXC3pDe/wu4XfJsl/R3gQETskvT+fg7pUXZBtbnwrojYL+lS4D5JT7zCvvPS5lR69P0sYJ6SlyRdAVD8PlCUJ/M6SKqQh/wfRsQ3i+Lk2w0QEUeAPwduIO02vwv4RUnPkg+3/i1JXyftNhMR+4vfB4C7yYdiFrTNqQT9UluEfDvwy8XtXwb+tK38JkkDkjYCm4AHFqF+50R51/33gJ9ExO+0bUq23ZLWFj15JA0BHwCeIOE2R8TnI2J9RGwg/8z+n4j4pyTcZknDklbM3AY+BDzKQrd5sb+Bnsdvsj9CfnbG08CvL3Z95rFdfwS8ANTI/7p/CrgE+C7wVPH74rb9f714DXYDNy52/c+yze8m/+/pw8CDxc9HUm43cDXw46LNjwJfKMqTbXNH+9/P7Fk3ybaZ/MzAh4qfx2ayaqHb7CkQzMwSl8rQjZmZnYaD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PE/X/Te/IabpfgRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params, losses = naive_train(\n",
    "    key, naive_forward_t, reps, labels, epochs=160, learning_rate=1e-3)\n",
    "#forward_t = hk.without_apply_rng(hk.transform(forward_fxn))\n",
    "#forward = functools.partial(forward_t.apply, params)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e(logits): # params is trained mlp params\n",
    "    s = alpdesign.SeqpropBlock()(logits)\n",
    "    us = alpdesign.seq2useq(s)\n",
    "    u = alpdesign.differentiable_jax_unirep(us)\n",
    "    return naive_forward(u)\n",
    "e2e_t = hk.transform(e2e)\n",
    "init_logits = jax.random.normal(key, shape=((10, 20)))\n",
    "e2e_params = e2e_t.init(key, init_logits)\n",
    "\n",
    "def e2e_fxn(x, key):\n",
    "    e2e_params, logits = x\n",
    "    yhat = e2e_t.apply(e2e_params, key, logits)\n",
    "    return yhat\n",
    "\n",
    "\n",
    "def gradient_opt(key, f, init_x=None, iter_num=500, learning_rate=1e-2):\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(init_x)\n",
    "    x = init_x\n",
    "    reduced_f = lambda *args: jnp.mean(f(*args))\n",
    "    \n",
    "    @jax.jit\n",
    "    def step(x, opt_state, key):\n",
    "        loss, g = jax.value_and_grad(reduced_f, 0)(x, key)\n",
    "        updates, opt_state = optimizer.update(g, opt_state)\n",
    "        x = optax.apply_updates(x, updates)\n",
    "        return x, opt_state, loss\n",
    "    losses = []\n",
    "    for step_idx in range(iter_num):\n",
    "        key, _ = jax.random.split(key, num=2)\n",
    "        x, opt_state, loss= step(x, opt_state, key)\n",
    "        losses.append(loss)\n",
    "            \n",
    "    return x, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "BO_batch_size = 16\n",
    "reps = get_reps(seqs)[0]\n",
    "labels = []\n",
    "for seq in seqs:\n",
    "    labels.append(blosum(target_seq, seq))\n",
    "labels = np.array(labels)\n",
    "def loop(key, reps, labels, params):\n",
    "    key, train_key = jax.random.split(key, num=2)\n",
    "    #reps = get_reps(seqs)[0]\n",
    "    '''\n",
    "    labels = []\n",
    "    for seq in seqs:\n",
    "        labels.append(blosum(target_seq, seq))\n",
    "    labels = np.array(labels)\n",
    "    '''\n",
    "    params, losses = naive_train(\n",
    "    key, naive_forward_t, reps, labels, epochs=500, learning_rate=1e-3)\n",
    "    #params, losses = alpdesign.ensemble_train(train_key, full_forward_t, c, reps, labels, params=params, epochs=100)\n",
    "    # make random point\n",
    "    init_logits = jax.random.normal(key, shape=((10, 20)))\n",
    "    key, train_key = jax.random.split(key, num=2)\n",
    "    init_x = 0.1*jax.random.normal(key, shape=(BO_batch_size, 10, 20))\n",
    "    #forward = jax.vmap(lambda x, key: forward_t.apply(params, x), in_axes=(0, None))\n",
    "    print('start Optimizing')\n",
    "    #batch_e2e = jax.vmap(lambda x, key: e2e_fxn(params, x), in_axes=((None, 0), None))\n",
    "    batch_e2e = jax.vmap(e2e_fxn, ((None, 0), None))\n",
    "    batched_x, losses = gradient_opt(key, batch_e2e, (e2e_t.init(train_key, init_logits), init_x))\n",
    "    top_idx = np.argmin(losses[-1])\n",
    "    rb = batched_x[0]\n",
    "    batched_logits = batched_x[1]\n",
    "    final_logits = batched_logits[top_idx]\n",
    "    vec = alpdesign.seq.forward_seqprop.apply(rb, key, final_logits)\n",
    "    s = decode_seq(vec)\n",
    "    reps = np.concatenate((reps, get_reps([s])[0]))\n",
    "    \n",
    "    y = blosum(target_seq, s)\n",
    "    print(reps.shape)\n",
    "    print(s, y)\n",
    "    labels = np.concatenate((labels, np.array(y).reshape(1,)))\n",
    "    '''\n",
    "    plt.figure()\n",
    "    proj_x = pca.transform(reps)\n",
    "    #print(proj_x)\n",
    "    plt.scatter(proj_x[:,0], proj_x[:,1], c=labels, cmap=plt.get_cmap('Reds'))\n",
    "    plt.colorbar()\n",
    "    proj_l = pca.transform(target_rep)\n",
    "    plt.plot(proj_l[:,0], proj_l[:,1], 'p')\n",
    "    plt.plot(proj_x[-1, 0], proj_x[-1, 1], 'o', color='green')    \n",
    "    plt.show()\n",
    "    \n",
    "    '''\n",
    "    return key, reps, labels, s, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "start BO\n",
      "(13, 1900)\n",
      "['P', 'C', 'C', 'A', 'C', 'C', 'A', 'C', 'A', 'C'] 0.3100255\n",
      "['P', 'C', 'C', 'A', 'C', 'C', 'A', 'C', 'A', 'C']\n",
      "1\n",
      "start BO\n",
      "(14, 1900)\n",
      "['C', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P'] 0.10909683\n",
      "['C', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P']\n",
      "2\n",
      "start BO\n",
      "(15, 1900)\n",
      "['C', 'A', 'A', 'P', 'A', 'A', 'P', 'A', 'P', 'P'] 0.24973989\n",
      "['C', 'A', 'A', 'P', 'A', 'A', 'P', 'A', 'P', 'P']\n",
      "3\n",
      "start BO\n",
      "(16, 1900)\n",
      "['C', 'P', 'P', 'P', 'A', 'P', 'P', 'A', 'P', 'P'] 0.15446526\n",
      "['C', 'P', 'P', 'P', 'A', 'P', 'P', 'A', 'P', 'P']\n",
      "4\n",
      "start BO\n",
      "(17, 1900)\n",
      "['C', 'P', 'P', 'P', 'A', 'P', 'P', 'P', 'P', 'P'] 0.13010848\n",
      "['C', 'P', 'P', 'P', 'A', 'P', 'P', 'P', 'P', 'P']\n",
      "5\n",
      "start BO\n",
      "(18, 1900)\n",
      "['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C'] 0.18242553\n",
      "['C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "6\n",
      "start BO\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "params = None\n",
    "for i in range(20):\n",
    "    print(i)\n",
    "    key, reps, labels, final_vec, params= loop(key, reps, labels, params)\n",
    "    print(final_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (alpdesign)",
   "language": "python",
   "name": "alpdesign"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
