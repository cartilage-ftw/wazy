{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpdesign.utils import *\n",
    "from alpdesign.mlp import *\n",
    "from jax_unirep import get_reps\n",
    "import alpdesign\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax_unirep\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import functools\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_list = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','S','T','W','Y','V','B','Z','X','*']\n",
    "blosum92 = np.loadtxt(\"./blosum62.txt\", dtype='i', delimiter=' ')\n",
    "avg92 = jnp.sum(blosum92)/24/24\n",
    "sum92 = 0.\n",
    "for row in blosum92:\n",
    "    for aa in row:\n",
    "        sum92 += (aa-avg92)**2\n",
    "std92 = jnp.sqrt(sum92 / 24/24)\n",
    "\n",
    "def blosum(seq1, seq2):\n",
    "    seqlist1 = list(seq1)\n",
    "    seqlist2 = list(seq2)\n",
    "    score = 0.\n",
    "    for i in range(len(seqlist1)):\n",
    "        idx1 = AA_list.index(seqlist1[i])\n",
    "        idx2 = AA_list.index(seqlist2[i])\n",
    "        score += (blosum92[idx1][idx2] - avg92)/std92\n",
    "        #jax.nn.sigmoid(score/len(seqlist1))\n",
    "    return score/len(seqlist1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01368617]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntarget_seq = 'GIGAVLKVLT'\\noh_vec = encode_seq(list(target_seq))\\noh_unirep = seq2useq(oh_vec)\\ntarget_rep = differentiable_jax_unirep(oh_unirep)\\n\\nseqs = ['GIGAVLKVLKAGLPALIVTLKRKIVQ',\\n       'PPGATLKKHTTGSVALISWIWARIQQ',\\n       'GIGAVLKVLTTGLKTLISAAKRKRAA',\\n       'HAPPVLKVLTTGLAPPLVWIKRKRTH',\\n       'GIGAVLUIHKLSSVAAWRPPKRKRQQ',\\n       'PTWIIFLKAQWEQHSNLTNMRTFPEV',\\n        'TISHFVCNHDICAWIKDMQAMQIKMC',\\n        'CESWLWKRLFDGHADRWRSMPDYPIW',\\n        'YLVENPLMFPLVAAFIHQWTRQISWH',\\n        'QTEERLEAQISIYYIGAWSHYKVTDE']\\n\\n\\nseqs = ['SSVAAWRPPK',\\n       'PPGATLKKHT',\\n       'LTGAVLKVLK',\\n       'HAPPVLKVLT',\\n       'AWRPPKRKRQ',\\n       'PTWIIFLKAQ',\\n       'TISHFVCNHD',\\n       'CESWLWKRLF',\\n       'YLVENPLMFP',\\n       'QTEERLEAQI',\\n       'GAWSHYKVTD',\\n       'HADRWRSMPD']\\nreps = get_reps(seqs)[0]\\nlabels = []\\nfor seq in seqs:\\n    labels.append(blosum(target_seq, seq))\\nlabels = np.array(labels)\\nprint(labels)\\n\\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seq = 'TARGETPEPTIDE'\n",
    "oh_vec = encode_seq(list(target_seq))\n",
    "oh_unirep = seq2useq(oh_vec)\n",
    "target_rep = differentiable_jax_unirep(oh_unirep)\n",
    "seqs = ['ISQLRYICEVIWF']\n",
    "reps = get_reps(seqs)[0]\n",
    "labels = []\n",
    "for seq in seqs:\n",
    "    labels.append(blosum(target_seq, seq))\n",
    "labels = np.array(labels)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "'''\n",
    "target_seq = 'GIGAVLKVLT'\n",
    "oh_vec = encode_seq(list(target_seq))\n",
    "oh_unirep = seq2useq(oh_vec)\n",
    "target_rep = differentiable_jax_unirep(oh_unirep)\n",
    "\n",
    "seqs = ['GIGAVLKVLKAGLPALIVTLKRKIVQ',\n",
    "       'PPGATLKKHTTGSVALISWIWARIQQ',\n",
    "       'GIGAVLKVLTTGLKTLISAAKRKRAA',\n",
    "       'HAPPVLKVLTTGLAPPLVWIKRKRTH',\n",
    "       'GIGAVLUIHKLSSVAAWRPPKRKRQQ',\n",
    "       'PTWIIFLKAQWEQHSNLTNMRTFPEV',\n",
    "        'TISHFVCNHDICAWIKDMQAMQIKMC',\n",
    "        'CESWLWKRLFDGHADRWRSMPDYPIW',\n",
    "        'YLVENPLMFPLVAAFIHQWTRQISWH',\n",
    "        'QTEERLEAQISIYYIGAWSHYKVTDE']\n",
    "\n",
    "\n",
    "seqs = ['SSVAAWRPPK',\n",
    "       'PPGATLKKHT',\n",
    "       'LTGAVLKVLK',\n",
    "       'HAPPVLKVLT',\n",
    "       'AWRPPKRKRQ',\n",
    "       'PTWIIFLKAQ',\n",
    "       'TISHFVCNHD',\n",
    "       'CESWLWKRLF',\n",
    "       'YLVENPLMFP',\n",
    "       'QTEERLEAQI',\n",
    "       'GAWSHYKVTD',\n",
    "       'HADRWRSMPD']\n",
    "reps = get_reps(seqs)[0]\n",
    "labels = []\n",
    "for seq in seqs:\n",
    "    labels.append(blosum(target_seq, seq))\n",
    "labels = np.array(labels)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "class NaiveBlock(hk.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = hk.nets.MLP((256, 128, 64, 1,))(x)\n",
    "        return out\n",
    "    \n",
    "def naive_forward(x):\n",
    "    f = NaiveBlock()\n",
    "    return f(x)\n",
    "naive_forward_t = hk.without_apply_rng(hk.transform(naive_forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _naive_loss(forward, params, seq, label):\n",
    "    yhat = forward(params, seq) #scalar\n",
    "    return (label-yhat)**2\n",
    "\n",
    "def _shuffle_in_unison(key, a, b):\n",
    "    # NOTE to future self: do not try to rely on keys being same\n",
    "    # something about shape of arrays makes shuffle not the same\n",
    "    assert len(a) == len(b)\n",
    "    p = jax.random.permutation(key, len(a))\n",
    "    return jnp.array([a[i] for i in p]), jnp.array([b[i] for i in p])\n",
    "\n",
    "\n",
    "def _fill_to_batch(x, y, key, batch_size):\n",
    "    if len(y) >= batch_size:\n",
    "        return x, y\n",
    "    i = jax.random.choice(key, jnp.arange(len(y)),\n",
    "                          shape=(batch_size,), replace=True)\n",
    "    x = x[i, ...]\n",
    "    y = y[i, ...]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def naive_train(key, forward_t, seqs, labels, val_seqs=None, val_labels=None, params=None, epochs=3, batch_size=8, learning_rate=1e-2):\n",
    "    opt_init, opt_update = optax.chain(\n",
    "        optax.scale_by_adam(b1=0.8, b2=0.9, eps=1e-4),\n",
    "        optax.scale(-learning_rate)  # minus sign -- minimizing the loss\n",
    "    )\n",
    "\n",
    "    key, bkey = jax.random.split(key)\n",
    "\n",
    "    # fill in seqs/labels if too small\n",
    "    seqs, labels = _fill_to_batch(seqs, labels, bkey, batch_size)\n",
    "   \n",
    "        \n",
    "    if params == None:\n",
    "        params = forward_t.init(key, seqs[0])\n",
    "  \n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    # wrap loss in batch/sum\n",
    "    loss_ = partial(_naive_loss, forward_t.apply)\n",
    "    loss_fxn = lambda *args: jnp.mean(jax.vmap(loss_, in_axes=(None, 0, 0))(*args))\n",
    "    #loss_fxn = jnp.mean(jax.vmap(loss_, in_axes=(None, 0, 0)))\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(opt_state, params, seqs, labels):\n",
    "        loss, grad = jax.value_and_grad(loss_fxn, 0)(\n",
    "            params, seqs, labels)\n",
    "        updates, opt_state = opt_update(grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return opt_state, params, loss\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for e in range(epochs):\n",
    "        # shuffle seqs and labels\n",
    "        key, key_ = jax.random.split(key, num=2)\n",
    "        shuffle_seqs, shuffle_labels = _shuffle_in_unison(key, seqs, labels)\n",
    "        for i in range(0, len(shuffle_labels) // batch_size):\n",
    "            seq = shuffle_seqs[i:(i+1) * batch_size]\n",
    "            label = shuffle_labels[i:(i+1) * batch_size]\n",
    "            opt_state, params, loss = train_step(opt_state, params, seq, label)\n",
    "            losses.append(loss)\n",
    "        # compute validation loss\n",
    "        if val_seqs is not None:\n",
    "            val_loss = 0.\n",
    "            for i in range(0, len(val_labels) // batch_size):\n",
    "                seq = shuffle_seqs[i:(i+1) * batch_size]\n",
    "                label = shuffle_seqs[i:(i+1) * batch_size]\n",
    "                val_loss += loss_fxn(\n",
    "                    params,\n",
    "                    val_seqs[i:(i+1) * batch_size],\n",
    "                    val_labels[i:(i+1) * batch_size])\n",
    "            val_loss = val_loss/len(val_labels) * batch_size\n",
    "            #batch_loss += loss\n",
    "            val_losses.append(val_loss)\n",
    "    return (params, losses) if val_seqs is None else (params, losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd34a79cd30>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params, losses = naive_train(\n",
    "    key, naive_forward_t, reps, labels, epochs=160, learning_rate=1e-3)\n",
    "#forward_t = hk.without_apply_rng(hk.transform(forward_fxn))\n",
    "#forward = functools.partial(forward_t.apply, params)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e(params, logits): # params is trained mlp params\n",
    "    s = alpdesign.SeqpropBlock()(logits)\n",
    "    us = alpdesign.seq2useq(s)\n",
    "    u = alpdesign.differentiable_jax_unirep(us)\n",
    "    forward = functools.partial(naive_forward_t.apply, params)\n",
    "    return forward(u)\n",
    "#e2e_t = hk.transform(e2e)\n",
    "#init_logits = jax.random.normal(key, shape=((10, 20)))\n",
    "#e2e_params = e2e_t.init(key, init_logits)\n",
    "\n",
    "def e2e_fxn(e2e_t, x, key):\n",
    "    e2e_params, logits = x\n",
    "    yhat = e2e_t.apply(e2e_params, key, logits)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_opt(key, f, init_x=None, iter_num=500, learning_rate=1e-2):\n",
    "    optimizer = optax.adam(-learning_rate)\n",
    "    opt_state = optimizer.init(init_x)\n",
    "    x = init_x\n",
    "    reduced_f = lambda *args: jnp.mean(f(*args))\n",
    "    \n",
    "    @jax.jit\n",
    "    def step(x, opt_state, key):\n",
    "        loss, g = jax.value_and_grad(reduced_f, 0)(x, key)\n",
    "        updates, opt_state = optimizer.update(g, opt_state)\n",
    "        x = optax.apply_updates(x, updates)\n",
    "        return x, opt_state, loss\n",
    "    losses = []\n",
    "    for step_idx in range(iter_num):\n",
    "        key, _ = jax.random.split(key, num=2)\n",
    "        x, opt_state, loss= step(x, opt_state, key)\n",
    "        losses.append(loss)\n",
    "            \n",
    "    return x, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "BO_batch_size = 16\n",
    "reps = get_reps(seqs)[0]\n",
    "labels = []\n",
    "for seq in seqs:\n",
    "    labels.append(blosum(target_seq, seq))\n",
    "labels = np.array(labels)\n",
    "def loop(key, reps, labels, params, rb=None):\n",
    "    key, train_key = jax.random.split(key, num=2)\n",
    "    params, mlp_loss= naive_train(key, naive_forward_t, reps, labels, params=params, epochs=160)\n",
    "    # make random point\n",
    "    init_logits = 0.1*jax.random.normal(key, shape=((13, 20)))\n",
    "    e2e_ = lambda logits: functools.partial(e2e, params)(logits)\n",
    "    e2e_t = hk.transform(e2e_)\n",
    "    key, train_key = jax.random.split(key, num=2)\n",
    "    if rb is None:\n",
    "        rb = e2e_t.init(key, init_logits)\n",
    "    e2e_params = rb\n",
    "    key, train_key = jax.random.split(key, num=2)\n",
    "    #init_x = 0.1*jax.random.normal(key, shape=(BO_batch_size, 10, 20))\n",
    "    #forward = jax.vmap(lambda x, key: forward_t.apply(params, x), in_axes=(0, None))\n",
    "    print('start optimizing')\n",
    "    #batch_e2e = jax.vmap(lambda x, key: functools.partial(e2e_fxn, e2e_t)(params, x), in_axes=((None, 0), None))\n",
    "    batch_e2e = functools.partial(e2e_fxn, e2e_t)\n",
    "    #batched_x, bo_losses = alpdesign.mlp.bayes_opt(key, batch_e2e, labels, (e2e_t.init(train_key, init_logits), init_x), epsilon=0.01, iter_num=500)\n",
    "    x, losses = gradient_opt(key, batch_e2e, (e2e_params, init_logits))\n",
    "    #top_idx = np.argmin(bo_losses[-1])\n",
    "    rb = x[0]\n",
    "    logits = x[1]\n",
    "    \n",
    "    vec = alpdesign.seq.forward_seqprop.apply(rb, key, logits)\n",
    "    s = decode_seq(vec)\n",
    "    reps = np.concatenate((reps, get_reps([s])[0]))\n",
    "    #print(get_reps([s])[0])\n",
    "    y = blosum(target_seq, s)\n",
    "    predicted_y = naive_forward_t.apply(params, get_reps([s])[0])\n",
    "    print(reps.shape)\n",
    "    print(s, y, predicted_y)\n",
    "    labels = np.concatenate((labels, np.array(y).reshape(1,)))\n",
    "    return key, reps, labels, s, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "start optimizing\n",
      "(102, 1900)\n",
      "['K', 'R', 'E', 'M', 'K', 'Q', 'R', 'R', 'N', 'E', 'Q', 'Q', 'E'] 0.29776552 [[0.48825538]]\n",
      "['K', 'R', 'E', 'M', 'K', 'Q', 'R', 'R', 'N', 'E', 'Q', 'Q', 'E']\n",
      "1\n",
      "start optimizing\n",
      "(103, 1900)\n",
      "['R', 'E', 'E', 'K', 'E', 'R', 'K', 'E', 'E', 'T', 'E', 'S', 'Q'] 0.83041435 [[0.5417428]]\n",
      "['R', 'E', 'E', 'K', 'E', 'R', 'K', 'E', 'E', 'T', 'E', 'S', 'Q']\n",
      "2\n",
      "start optimizing\n",
      "(104, 1900)\n",
      "['M', 'D', 'R', 'E', 'A', 'E', 'T', 'K', 'R', 'D', 'P', 'N', 'A'] 0.29776552 [[0.38781047]]\n",
      "['M', 'D', 'R', 'E', 'A', 'E', 'T', 'K', 'R', 'D', 'P', 'N', 'A']\n",
      "3\n",
      "start optimizing\n",
      "(105, 1900)\n",
      "['K', 'K', 'R', 'A', 'R', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E'] 0.8659243 [[0.6793434]]\n",
      "['K', 'K', 'R', 'A', 'R', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E']\n",
      "4\n",
      "start optimizing\n",
      "(106, 1900)\n",
      "['P', 'K', 'A', 'E', 'E', 'A', 'E', 'A', 'Q', 'Q', 'Q', 'E', 'E'] 0.58184487 [[0.5328597]]\n",
      "['P', 'K', 'A', 'E', 'E', 'A', 'E', 'A', 'Q', 'Q', 'Q', 'E', 'E']\n",
      "5\n",
      "start optimizing\n",
      "(107, 1900)\n",
      "['T', 'K', 'E', 'A', 'N', 'E', 'E', 'P', 'K', 'T', 'E', 'T', 'T'] 0.5818449 [[0.90090847]]\n",
      "['T', 'K', 'E', 'A', 'N', 'E', 'E', 'P', 'K', 'T', 'E', 'T', 'T']\n",
      "6\n",
      "start optimizing\n",
      "(108, 1900)\n",
      "['M', 'L', 'A', 'R', 'K', 'M', 'S', 'E', 'E', 'Q', 'Q', 'Q', 'D'] 0.43980518 [[0.29544643]]\n",
      "['M', 'L', 'A', 'R', 'K', 'M', 'S', 'E', 'E', 'Q', 'Q', 'Q', 'D']\n",
      "7\n",
      "start optimizing\n",
      "(109, 1900)\n",
      "['G', 'Q', 'E', 'R', 'E', 'R', 'E', 'P', 'E', 'R', 'S', 'P', 'E'] 0.47531512 [[0.68258566]]\n",
      "['G', 'Q', 'E', 'R', 'E', 'R', 'E', 'P', 'E', 'R', 'S', 'P', 'E']\n",
      "8\n",
      "start optimizing\n",
      "(110, 1900)\n",
      "['D', 'K', 'E', 'K', 'M', 'H', 'E', 'S', 'R', 'M', 'S', 'K', 'E'] 0.22674567 [[0.37044966]]\n",
      "['D', 'K', 'E', 'K', 'M', 'H', 'E', 'S', 'R', 'M', 'S', 'K', 'E']\n",
      "9\n",
      "start optimizing\n",
      "(111, 1900)\n",
      "['M', 'A', 'E', 'R', 'P', 'A', 'S', 'A', 'P', 'S', 'E', 'P', 'P'] 0.61735487 [[0.69877553]]\n",
      "['M', 'A', 'E', 'R', 'P', 'A', 'S', 'A', 'P', 'S', 'E', 'P', 'P']\n",
      "10\n",
      "start optimizing\n",
      "(112, 1900)\n",
      "['M', 'V', 'K', 'S', 'E', 'A', 'P', 'Q', 'P', 'S', 'P', 'P', 'A'] 1.2210236 [[0.6317981]]\n",
      "['M', 'V', 'K', 'S', 'E', 'A', 'P', 'Q', 'P', 'S', 'P', 'P', 'A']\n",
      "11\n",
      "start optimizing\n",
      "(113, 1900)\n",
      "['N', 'S', 'Q', 'E', 'I', 'H', 'E', 'L', 'S', 'D', 'Q', 'A', 'Q'] 0.084706016 [[0.27716666]]\n",
      "['N', 'S', 'Q', 'E', 'I', 'H', 'E', 'L', 'S', 'D', 'Q', 'A', 'Q']\n",
      "12\n",
      "start optimizing\n",
      "(114, 1900)\n",
      "['P', 'A', 'K', 'P', 'H', 'E', 'K', 'S', 'C', 'P', 'I', 'Q', 'Q'] 0.68837464 [[0.16276743]]\n",
      "['P', 'A', 'K', 'P', 'H', 'E', 'K', 'S', 'C', 'P', 'I', 'Q', 'Q']\n",
      "13\n",
      "start optimizing\n",
      "(115, 1900)\n",
      "['M', 'V', 'M', 'A', 'S', 'E', 'K', 'A', 'Q', 'P', 'Q', 'S', 'P'] 0.19123575 [[0.8937265]]\n",
      "['M', 'V', 'M', 'A', 'S', 'E', 'K', 'A', 'Q', 'P', 'Q', 'S', 'P']\n",
      "14\n",
      "start optimizing\n",
      "(116, 1900)\n",
      "['W', 'K', 'M', 'H', 'P', 'T', 'E', 'E', 'Q', 'R', 'Q', 'Q', 'Q'] 0.5463349 [[0.49566302]]\n",
      "['W', 'K', 'M', 'H', 'P', 'T', 'E', 'E', 'Q', 'R', 'Q', 'Q', 'Q']\n",
      "15\n",
      "start optimizing\n",
      "(117, 1900)\n",
      "['E', 'Q', 'E', 'A', 'R', 'R', 'E', 'K', 'E', 'Q', 'E', 'Q', 'Q'] 0.36878535 [[0.59298515]]\n",
      "['E', 'Q', 'E', 'A', 'R', 'R', 'E', 'K', 'E', 'Q', 'E', 'Q', 'Q']\n",
      "16\n",
      "start optimizing\n",
      "(118, 1900)\n",
      "['T', 'A', 'E', 'K', 'A', 'E', 'T', 'A', 'P', 'Q', 'P', 'S', 'N'] 0.79490453 [[1.0921946]]\n",
      "['T', 'A', 'E', 'K', 'A', 'E', 'T', 'A', 'P', 'Q', 'P', 'S', 'N']\n",
      "17\n",
      "start optimizing\n",
      "(119, 1900)\n",
      "['G', 'E', 'K', 'E', 'K', 'E', 'S', 'E', 'D', 'E', 'D', 'E', 'Q'] 0.58184487 [[0.73994935]]\n",
      "['G', 'E', 'K', 'E', 'K', 'E', 'S', 'E', 'D', 'E', 'D', 'E', 'Q']\n",
      "18\n",
      "start optimizing\n",
      "(120, 1900)\n",
      "['M', 'K', 'E', 'K', 'S', 'K', 'V', 'Q', 'P', 'A', 'P', 'Q', 'P'] 0.51082504 [[0.90981746]]\n",
      "['M', 'K', 'E', 'K', 'S', 'K', 'V', 'Q', 'P', 'A', 'P', 'Q', 'P']\n",
      "19\n",
      "start optimizing\n",
      "(121, 1900)\n",
      "['L', 'E', 'T', 'Q', 'K', 'Q', 'S', 'E', 'K', 'E', 'E', 'E', 'E'] 0.6173548 [[0.39771467]]\n",
      "['L', 'E', 'T', 'Q', 'K', 'Q', 'S', 'E', 'K', 'E', 'E', 'E', 'E']\n",
      "20\n",
      "start optimizing\n",
      "(122, 1900)\n",
      "['P', 'K', 'K', 'E', 'T', 'Q', 'S', 'A', 'P', 'P', 'P', 'A', 'A'] 0.36878535 [[0.7671112]]\n",
      "['P', 'K', 'K', 'E', 'T', 'Q', 'S', 'A', 'P', 'P', 'P', 'A', 'A']\n",
      "21\n",
      "start optimizing\n",
      "(123, 1900)\n",
      "['Q', 'A', 'S', 'Q', 'I', 'S', 'W', 'Q', 'Q', 'S', 'T', 'Q', 'K'] 0.43980518 [[0.08735087]]\n",
      "['Q', 'A', 'S', 'Q', 'I', 'S', 'W', 'Q', 'Q', 'S', 'T', 'Q', 'K']\n",
      "22\n",
      "start optimizing\n",
      "(124, 1900)\n",
      "['R', 'P', 'P', 'A', 'E', 'E', 'E', 'E', 'P', 'E', 'S', 'E', 'Q'] 1.0079639 [[0.95830494]]\n",
      "['R', 'P', 'P', 'A', 'E', 'E', 'E', 'E', 'P', 'E', 'S', 'E', 'Q']\n",
      "23\n",
      "start optimizing\n",
      "(125, 1900)\n",
      "['V', 'E', 'K', 'A', 'Q', 'E', 'E', 'A', 'Q', 'S', 'E', 'E', 'E'] 0.7238845 [[0.59310555]]\n",
      "['V', 'E', 'K', 'A', 'Q', 'E', 'E', 'A', 'Q', 'S', 'E', 'E', 'E']\n",
      "24\n",
      "start optimizing\n",
      "(126, 1900)\n",
      "['M', 'K', 'K', 'W', 'S', 'S', 'Q', 'E', 'R', 'Q', 'A', 'E', 'D'] 0.6883747 [[0.46250528]]\n",
      "['M', 'K', 'K', 'W', 'S', 'S', 'Q', 'E', 'R', 'Q', 'A', 'E', 'D']\n",
      "25\n",
      "start optimizing\n",
      "(127, 1900)\n",
      "['V', 'K', 'A', 'Q', 'Q', 'E', 'E', 'E', 'E', 'K', 'R', 'T', 'Q'] 0.47531512 [[0.6155785]]\n",
      "['V', 'K', 'A', 'Q', 'Q', 'E', 'E', 'E', 'E', 'K', 'R', 'T', 'Q']\n",
      "26\n",
      "start optimizing\n",
      "(128, 1900)\n",
      "['Q', 'V', 'P', 'Q', 'K', 'K', 'K', 'P', 'W', 'P', 'S', 'A', 'E'] 0.19123575 [[0.6828947]]\n",
      "['Q', 'V', 'P', 'Q', 'K', 'K', 'K', 'P', 'W', 'P', 'S', 'A', 'E']\n",
      "27\n",
      "start optimizing\n",
      "(129, 1900)\n",
      "['M', 'K', 'S', 'R', 'C', 'P', 'P', 'P', 'A', 'P', 'A', 'P', 'N'] 0.29776552 [[1.0071715]]\n",
      "['M', 'K', 'S', 'R', 'C', 'P', 'P', 'P', 'A', 'P', 'A', 'P', 'N']\n",
      "28\n",
      "start optimizing\n",
      "(130, 1900)\n",
      "['G', 'G', 'R', 'R', 'E', 'Q', 'P', 'A', 'E', 'P', 'Q', 'Q', 'E'] 0.97245413 [[0.5821455]]\n",
      "['G', 'G', 'R', 'R', 'E', 'Q', 'P', 'A', 'E', 'P', 'Q', 'Q', 'E']\n",
      "29\n",
      "start optimizing\n",
      "(131, 1900)\n",
      "['E', 'E', 'E', 'A', 'A', 'A', 'A', 'A', 'K', 'A', 'E', 'A', 'A'] 0.15572584 [[0.56080174]]\n",
      "['E', 'E', 'E', 'A', 'A', 'A', 'A', 'A', 'K', 'A', 'E', 'A', 'A']\n",
      "30\n",
      "start optimizing\n",
      "(132, 1900)\n",
      "['M', 'K', 'K', 'M', 'R', 'H', 'V', 'S', 'R', 'V', 'R', 'K', 'S'] 0.120215915 [[0.40740502]]\n",
      "['M', 'K', 'K', 'M', 'R', 'H', 'V', 'S', 'R', 'V', 'R', 'K', 'S']\n",
      "31\n",
      "start optimizing\n",
      "(133, 1900)\n",
      "['Q', 'A', 'P', 'E', 'E', 'E', 'Y', 'G', 'Q', 'T', 'R', 'N', 'R'] 0.5818449 [[0.15601952]]\n",
      "['Q', 'A', 'P', 'E', 'E', 'E', 'Y', 'G', 'Q', 'T', 'R', 'N', 'R']\n",
      "32\n",
      "start optimizing\n",
      "(134, 1900)\n",
      "['Y', 'E', 'A', 'E', 'W', 'P', 'Q', 'A', 'E', 'E', 'E', 'E', 'D'] 0.1202159 [[0.5469462]]\n",
      "['Y', 'E', 'A', 'E', 'W', 'P', 'Q', 'A', 'E', 'E', 'E', 'E', 'D']\n",
      "33\n",
      "start optimizing\n",
      "(135, 1900)\n",
      "['E', 'K', 'T', 'K', 'Q', 'T', 'E', 'P', 'A', 'T', 'Q', 'E', 'Q'] 0.7593945 [[0.5395303]]\n",
      "['E', 'K', 'T', 'K', 'Q', 'T', 'E', 'P', 'A', 'T', 'Q', 'E', 'Q']\n",
      "34\n",
      "start optimizing\n",
      "(136, 1900)\n",
      "['E', 'V', 'T', 'E', 'E', 'T', 'Q', 'N', 'Q', 'E', 'T', 'D', 'Q'] 0.9369441 [[0.57857794]]\n",
      "['E', 'V', 'T', 'E', 'E', 'T', 'Q', 'N', 'Q', 'E', 'T', 'D', 'Q']\n",
      "35\n",
      "start optimizing\n",
      "(137, 1900)\n",
      "['E', 'T', 'Q', 'P', 'V', 'E', 'P', 'T', 'E', 'Q', 'T', 'D', 'Q'] 0.7949045 [[0.9573105]]\n",
      "['E', 'T', 'Q', 'P', 'V', 'E', 'P', 'T', 'E', 'Q', 'T', 'D', 'Q']\n",
      "36\n",
      "start optimizing\n",
      "(138, 1900)\n",
      "['Y', 'V', 'K', 'T', 'E', 'A', 'P', 'Q', 'E', 'Q', 'T', 'T', 'E'] 1.043474 [[0.5494268]]\n",
      "['Y', 'V', 'K', 'T', 'E', 'A', 'P', 'Q', 'E', 'Q', 'T', 'T', 'E']\n",
      "37\n",
      "start optimizing\n",
      "(139, 1900)\n",
      "['R', 'V', 'K', 'T', 'E', 'A', 'P', 'A', 'E', 'E', 'E', 'E', 'E'] 1.0079641 [[1.0991389]]\n",
      "['R', 'V', 'K', 'T', 'E', 'A', 'P', 'A', 'E', 'E', 'E', 'E', 'E']\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "y = []\n",
    "yhat = []\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    rb = None\n",
    "    params = None\n",
    "    key, reps, labels, final_vec, params= loop(key, reps, labels, params)\n",
    "    print(final_vec)\n",
    "    y.append(blosum(target_seq, final_vec))\n",
    "    yhat.append(naive_forward_t.apply(params, get_reps([final_vec])[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y,label='y')\n",
    "plt.plot(yhat, label='yhat')\n",
    "plt.legend()\n",
    "plt.title('baseline(max)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (alp)",
   "language": "python",
   "name": "alp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
